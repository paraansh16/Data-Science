{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e7mIUFUO5jp6",
        "outputId": "9330388c-2e2f-4f0e-95b8-bea3bf9cf782"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Naive Bayes Accuracy: 0.8221\n",
            "Random Forest Accuracy: 1.0000\n",
            "Decision Tree Accuracy: 0.9556\n",
            "SVM Accuracy: 0.9532\n",
            "KNN Accuracy (Iris): 0.9556\n",
            "XGBoost Accuracy (Wine): 0.9815\n",
            "AdaBoost Accuracy (Iris): 0.9556\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/xgboost/core.py:158: UserWarning: [13:46:33] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/ensemble/_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "# Import necessary libraries\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.datasets import load_iris, load_wine, load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from xgboost import XGBClassifier\n",
        "\n",
        "# 1. Naive Bayes (NB) - Text Classification Example\n",
        "from sklearn.datasets import fetch_20newsgroups\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "# Load text data\n",
        "data = fetch_20newsgroups(subset='train')\n",
        "X, y = data.data, data.target\n",
        "\n",
        "# Convert text to feature vectors\n",
        "vectorizer = CountVectorizer()\n",
        "X_vectorized = vectorizer.fit_transform(X)\n",
        "\n",
        "# Train-test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_vectorized, y, test_size=0.3)\n",
        "\n",
        "# Train Naive Bayes Classifier\n",
        "nb = MultinomialNB()\n",
        "nb.fit(X_train, y_train)\n",
        "\n",
        "# Evaluate model\n",
        "accuracy_nb = nb.score(X_test, y_test)\n",
        "print(f\"Naive Bayes Accuracy: {accuracy_nb:.4f}\")\n",
        "\n",
        "# 2. Random Forest (RF) - Wine Classification\n",
        "# Load dataset\n",
        "wine = load_wine()\n",
        "X, y = wine.data, wine.target\n",
        "\n",
        "# Train-test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)\n",
        "\n",
        "# Train Random Forest Classifier\n",
        "rf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "rf.fit(X_train, y_train)\n",
        "\n",
        "# Evaluate model\n",
        "accuracy_rf = rf.score(X_test, y_test)\n",
        "print(f\"Random Forest Accuracy: {accuracy_rf:.4f}\")\n",
        "\n",
        "# 3. Decision Tree (DT) - Iris Classification\n",
        "# Load dataset\n",
        "iris = load_iris()\n",
        "X, y = iris.data, iris.target\n",
        "\n",
        "# Train-test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)\n",
        "\n",
        "# Train Decision Tree Classifier\n",
        "dt = DecisionTreeClassifier()\n",
        "dt.fit(X_train, y_train)\n",
        "\n",
        "# Evaluate model\n",
        "accuracy_dt = dt.score(X_test, y_test)\n",
        "print(f\"Decision Tree Accuracy: {accuracy_dt:.4f}\")\n",
        "\n",
        "# 4. Support Vector Machine (SVM) - Cancer Classification\n",
        "# Load dataset\n",
        "cancer = load_breast_cancer()\n",
        "X, y = cancer.data, cancer.target\n",
        "\n",
        "# Train-test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)\n",
        "\n",
        "# Train SVM Classifier\n",
        "svm = SVC(kernel='linear')\n",
        "svm.fit(X_train, y_train)\n",
        "\n",
        "# Evaluate model\n",
        "accuracy_svm = svm.score(X_test, y_test)\n",
        "print(f\"SVM Accuracy: {accuracy_svm:.4f}\")\n",
        "\n",
        "# 5. K-Nearest Neighbors (KNN) - Iris Classification\n",
        "# Load dataset\n",
        "iris = load_iris()\n",
        "X, y = iris.data, iris.target\n",
        "\n",
        "# Train-test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)\n",
        "\n",
        "# Train KNN Classifier\n",
        "knn = KNeighborsClassifier(n_neighbors=5)\n",
        "knn.fit(X_train, y_train)\n",
        "\n",
        "# Evaluate model\n",
        "accuracy_knn = knn.score(X_test, y_test)\n",
        "print(f\"KNN Accuracy (Iris): {accuracy_knn:.4f}\")\n",
        "\n",
        "# 6. XGBoost (Extreme Gradient Boosting) - Wine Classification\n",
        "# Load dataset\n",
        "wine = load_wine()\n",
        "X, y = wine.data, wine.target\n",
        "\n",
        "# Train-test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)\n",
        "\n",
        "# Train XGBoost Classifier\n",
        "xgb = XGBClassifier(use_label_encoder=False, eval_metric='mlogloss')\n",
        "xgb.fit(X_train, y_train)\n",
        "\n",
        "# Evaluate model\n",
        "accuracy_xgb = xgb.score(X_test, y_test)\n",
        "print(f\"XGBoost Accuracy (Wine): {accuracy_xgb:.4f}\")\n",
        "\n",
        "# 7. AdaBoost (Adaptive Boosting) - Iris Classification\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "\n",
        "# Train-test split\n",
        "X_train_iris, X_test_iris, y_train_iris, y_test_iris = train_test_split(iris.data, iris.target, test_size=0.3)\n",
        "\n",
        "# Train AdaBoost Classifier\n",
        "ada_iris = AdaBoostClassifier(DecisionTreeClassifier(max_depth=1), n_estimators=50)\n",
        "ada_iris.fit(X_train_iris, y_train_iris)\n",
        "\n",
        "# Evaluate model\n",
        "ada_accuracy_iris = ada_iris.score(X_test_iris, y_test_iris)\n",
        "print(f\"AdaBoost Accuracy (Iris): {ada_accuracy_iris:.4f}\")\n"
      ]
    }
  ]
}